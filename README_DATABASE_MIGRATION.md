# DATABASE RESTRUCTURING GUIDE

## üöÄ MIGRATION STEPS

### STEP 1: Create New Tables (5 minutes)
1. Open Supabase SQL Editor
2. Copy and paste the contents of `scripts/create-new-tables.sql`
3. Run the SQL to create the 4 new clean tables

### STEP 2: Migrate Data (5 minutes)
```bash
node scripts/migrate-data.js
```

### STEP 3: Verify Migration
Check the output - you should see:
- `raw_jobs`: ~1000+ records
- `processed_jobs`: ~700+ records  
- `identified_companies`: ~700+ records
- `search_terms_clean`: 37 records

---

## üìä NEW DATABASE STRUCTURE

### 1Ô∏è‚É£ **raw_jobs** - All scraped job postings
```sql
- job_id (PRIMARY KEY) - LinkedIn job ID
- platform - 'LinkedIn'
- company - Company name
- job_title - Job position
- location - Job location
- description - Full job description
- job_url - Link to job posting
- scraped_date - When we scraped it
- search_term - Which term found this job
- processed - TRUE/FALSE if analyzed
- processed_date - When analyzed
```

### 2Ô∏è‚É£ **processed_jobs** - Simple tracking
```sql
- job_id (PRIMARY KEY) - Links to raw_jobs
- processed_date - When processed
```

### 3Ô∏è‚É£ **identified_companies** - Tool users only
```sql
- id (PRIMARY KEY)
- company_name - Company name
- tool_detected - 'Outreach.io', 'SalesLoft', 'Both'
- signal_type - 'required', 'preferred', etc.
- context - Quote from job description
- confidence - 'high', 'medium', 'low'
- job_title - Job that detected this
- job_url - Link to job
- linkedin_url - For BDR manual entry
- platform - 'LinkedIn'
- identified_date - When detected
```

### 4Ô∏è‚É£ **search_terms_clean** - Weekly scraping tracking
```sql
- search_term (PRIMARY KEY) - Search term
- last_scraped_date - Last scrape date
- jobs_found_count - Jobs found in last scrape
- is_active - TRUE/FALSE for scraping
```

---

## üîß NEXT STEPS AFTER MIGRATION

1. **Update ScraperService** - Insert into `raw_jobs` instead of `job_queue`
2. **Update AnalysisService** - Read from `raw_jobs WHERE processed=FALSE`
3. **Update DataService** - Query new tables
4. **Update Dashboard** - Point to `identified_companies`
5. **Drop old tables** - After everything works

---

## üéØ BENEFITS OF NEW STRUCTURE

‚úÖ **10x Faster Queries** - No more JSONB parsing
‚úÖ **Crystal Clear Data** - Each table has one purpose
‚úÖ **Easy BDR Workflow** - LinkedIn URL ready for manual entry
‚úÖ **Simple Analysis Pipeline** - `WHERE processed=FALSE`
‚úÖ **Scalable Architecture** - Can handle millions of jobs
‚úÖ **Clean Weekly Tracking** - Know exactly what needs scraping

The migration preserves all existing data while organizing it properly!