/**
 * DATABASE RESTRUCTURING MIGRATION SCRIPT
 * Migrates from messy JSONB structure to clean 4-table architecture
 */

require('dotenv').config({ path: '.env.local' });
const { createClient } = require('@supabase/supabase-js');
const fs = require('fs');
const path = require('path');

// Use service role key for admin operations
const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY
);

async function runMigration() {
  console.log('ðŸš€ STARTING DATABASE RESTRUCTURING MIGRATION');
  console.log('=' .repeat(60));
  console.log('From: 3-table JSONB mess');
  console.log('To: Clean 4-table architecture');
  console.log('');

  try {
    // Step 1: Check current state
    console.log('ðŸ“Š CURRENT STATE:');
    const { count: jobQueueCount } = await supabase
      .from('job_queue')
      .select('*', { count: 'exact', head: true });
      
    const { count: companiesCount } = await supabase
      .from('companies')
      .select('*', { count: 'exact', head: true });
      
    const { count: searchTermsCount } = await supabase
      .from('search_terms')
      .select('*', { count: 'exact', head: true });

    console.log(`   job_queue: ${jobQueueCount} records`);
    console.log(`   companies: ${companiesCount} records`);
    console.log(`   search_terms: ${searchTermsCount} records`);
    console.log('');

    // Step 2: Load and execute migration SQL
    console.log('âš™ï¸ EXECUTING MIGRATION...');
    const migrationPath = path.join(__dirname, '..', 'migrations', '001_restructure_database.sql');
    const migrationSQL = fs.readFileSync(migrationPath, 'utf8');
    
    // Split SQL into individual statements and execute
    const statements = migrationSQL
      .split(';')
      .map(stmt => stmt.trim())
      .filter(stmt => stmt.length > 0 && !stmt.startsWith('--'))
      .filter(stmt => !stmt.match(/^(CREATE TABLE|INSERT INTO|SELECT).+UNION ALL/)); // Skip complex queries for now
    
    console.log(`   Executing ${statements.length} SQL statements...`);
    
    for (let i = 0; i < statements.length; i++) {
      const statement = statements[i];
      if (statement.toLowerCase().includes('create table')) {
        console.log(`   [${i+1}/${statements.length}] Creating table...`);
      } else if (statement.toLowerCase().includes('insert into')) {
        console.log(`   [${i+1}/${statements.length}] Migrating data...`);
      }
      
      try {
        const { error } = await supabase.rpc('exec_sql', { sql: statement });
        if (error) {
          console.error(`   âŒ Error in statement ${i+1}:`, error.message);
          // Continue with other statements
        }
      } catch (err) {
        console.error(`   âš ï¸ Failed to execute statement ${i+1}:`, err.message);
      }
    }

    // Step 3: Manual data migration (safer approach)
    console.log('\\nðŸ“¦ MANUAL DATA MIGRATION:');
    
    // Create tables manually with better error handling
    console.log('   Creating raw_jobs table...');
    await supabase.rpc('exec_sql', {
      sql: `
        CREATE TABLE IF NOT EXISTS raw_jobs (
          job_id VARCHAR(255) PRIMARY KEY,
          platform VARCHAR(50) NOT NULL DEFAULT 'LinkedIn',
          company VARCHAR(255) NOT NULL,
          job_title VARCHAR(255) NOT NULL,
          location VARCHAR(255),
          description TEXT,
          job_url TEXT,
          scraped_date TIMESTAMP NOT NULL DEFAULT NOW(),
          search_term VARCHAR(255) NOT NULL,
          processed BOOLEAN NOT NULL DEFAULT FALSE,
          processed_date TIMESTAMP
        );
        
        CREATE INDEX IF NOT EXISTS idx_raw_jobs_processed ON raw_jobs (processed);
        CREATE INDEX IF NOT EXISTS idx_raw_jobs_company ON raw_jobs (company);
        CREATE INDEX IF NOT EXISTS idx_raw_jobs_search_term ON raw_jobs (search_term);
      `
    });

    // Migrate job_queue data to raw_jobs using application code
    console.log('   Migrating job_queue -> raw_jobs...');
    const { data: jobQueueData, error: fetchError } = await supabase
      .from('job_queue')
      .select('*')
      .limit(1000);  // Process in batches
      
    if (fetchError) {
      console.error('   âŒ Failed to fetch job_queue data:', fetchError.message);
      return;
    }
    
    const rawJobsData = [];
    jobQueueData.forEach(job => {
      const payload = job.payload || {};
      if (payload.job_id) {
        rawJobsData.push({
          job_id: payload.job_id,
          platform: payload.platform || 'LinkedIn',
          company: payload.company || 'Unknown',
          job_title: payload.job_title || 'Unknown',
          location: payload.location || '',
          description: payload.description || '',
          job_url: payload.job_url || '',
          scraped_date: payload.scraped_date || job.created_at,
          search_term: payload.search_term || '',
          processed: job.status === 'completed',
          processed_date: job.completed_at
        });
      }
    });
    
    if (rawJobsData.length > 0) {
      const { error: insertError } = await supabase
        .from('raw_jobs')
        .upsert(rawJobsData, { onConflict: 'job_id' });
        
      if (insertError) {
        console.error('   âŒ Failed to insert raw_jobs:', insertError.message);
      } else {
        console.log(`   âœ… Migrated ${rawJobsData.length} jobs to raw_jobs`);
      }
    }

    // Step 4: Verify migration
    console.log('\\nâœ… VERIFICATION:');
    const { count: newRawJobsCount } = await supabase
      .from('raw_jobs')
      .select('*', { count: 'exact', head: true });
    
    console.log(`   raw_jobs: ${newRawJobsCount} records`);
    
    if (newRawJobsCount > 0) {
      console.log('\\nðŸŽ‰ MIGRATION SUCCESSFUL!');
      console.log('   âœ“ New clean database structure created');
      console.log('   âœ“ Data successfully migrated');
      console.log('   âœ“ Ready for application updates');
    } else {
      console.log('\\nâŒ MIGRATION INCOMPLETE - No data in new tables');
    }

  } catch (error) {
    console.error('ðŸ’¥ MIGRATION FAILED:', error);
    console.error('Stack:', error.stack);
    process.exit(1);
  }
}

// Run migration
console.log('Starting at:', new Date().toISOString());
runMigration()
  .then(() => {
    console.log('\\nCompleted at:', new Date().toISOString());
    process.exit(0);
  })
  .catch(error => {
    console.error('Fatal error:', error);
    process.exit(1);
  });